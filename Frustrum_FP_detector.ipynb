{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frustrum_FP_detector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZSZduF-lTQzFXbIMZ8o-hNPlEh-9GbHN",
      "authorship_tag": "ABX9TyMa1byukdN2V2+MnBlmxakv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtyomGrachev/defeater/blob/fp-pointnet-generator/Frustrum_FP_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIefoIgbTt_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!apt-get install tree\n",
        "!pip install importlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI3un5wwT4V8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import first part of train dataset from google cloud\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JyOyQk_UUOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!gsutil cp \tgs://waymo_open_dataset_v_1_0_0/training/training_0001.tar /content/training_data_0001.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-neLzueSUUsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy train 0001 tar into google drive\n",
        "\n",
        "#!gsutil cp gs://waymo_open_dataset_v_1_0_0/training/training_0001.tar /content/drive/'My Drive'/Waymo_OD/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbC-KidQUfWA",
        "colab_type": "code",
        "outputId": "ad71e87e-b34b-49a2-f59f-f06c560bcab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Clone github repo with util functions\n",
        "\n",
        "!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'waymo-od'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/98)\u001b[K\rremote: Counting objects:   2% (2/98)\u001b[K\rremote: Counting objects:   3% (3/98)\u001b[K\rremote: Counting objects:   4% (4/98)\u001b[K\rremote: Counting objects:   5% (5/98)\u001b[K\rremote: Counting objects:   6% (6/98)\u001b[K\rremote: Counting objects:   7% (7/98)\u001b[K\rremote: Counting objects:   8% (8/98)\u001b[K\rremote: Counting objects:   9% (9/98)\u001b[K\rremote: Counting objects:  10% (10/98)\u001b[K\rremote: Counting objects:  11% (11/98)\u001b[K\rremote: Counting objects:  12% (12/98)\u001b[K\rremote: Counting objects:  13% (13/98)\u001b[K\rremote: Counting objects:  14% (14/98)\u001b[K\rremote: Counting objects:  15% (15/98)\u001b[K\rremote: Counting objects:  16% (16/98)\u001b[K\rremote: Counting objects:  17% (17/98)\u001b[K\rremote: Counting objects:  18% (18/98)\u001b[K\rremote: Counting objects:  19% (19/98)\u001b[K\rremote: Counting objects:  20% (20/98)\u001b[K\rremote: Counting objects:  21% (21/98)\u001b[K\rremote: Counting objects:  22% (22/98)\u001b[K\rremote: Counting objects:  23% (23/98)\u001b[K\rremote: Counting objects:  24% (24/98)\u001b[K\rremote: Counting objects:  25% (25/98)\u001b[K\rremote: Counting objects:  26% (26/98)\u001b[K\rremote: Counting objects:  27% (27/98)\u001b[K\rremote: Counting objects:  28% (28/98)\u001b[K\rremote: Counting objects:  29% (29/98)\u001b[K\rremote: Counting objects:  30% (30/98)\u001b[K\rremote: Counting objects:  31% (31/98)\u001b[K\rremote: Counting objects:  32% (32/98)\u001b[K\rremote: Counting objects:  33% (33/98)\u001b[K\rremote: Counting objects:  34% (34/98)\u001b[K\rremote: Counting objects:  35% (35/98)\u001b[K\rremote: Counting objects:  36% (36/98)\u001b[K\rremote: Counting objects:  37% (37/98)\u001b[K\rremote: Counting objects:  38% (38/98)\u001b[K\rremote: Counting objects:  39% (39/98)\u001b[K\rremote: Counting objects:  40% (40/98)\u001b[K\rremote: Counting objects:  41% (41/98)\u001b[K\rremote: Counting objects:  42% (42/98)\u001b[K\rremote: Counting objects:  43% (43/98)\u001b[K\rremote: Counting objects:  44% (44/98)\u001b[K\rremote: Counting objects:  45% (45/98)\u001b[K\rremote: Counting objects:  46% (46/98)\u001b[K\rremote: Counting objects:  47% (47/98)\u001b[K\rremote: Counting objects:  48% (48/98)\u001b[K\rremote: Counting objects:  50% (49/98)\u001b[K\rremote: Counting objects:  51% (50/98)\u001b[K\rremote: Counting objects:  52% (51/98)\u001b[K\rremote: Counting objects:  53% (52/98)\u001b[K\rremote: Counting objects:  54% (53/98)\u001b[K\rremote: Counting objects:  55% (54/98)\u001b[K\rremote: Counting objects:  56% (55/98)\u001b[K\rremote: Counting objects:  57% (56/98)\u001b[K\rremote: Counting objects:  58% (57/98)\u001b[K\rremote: Counting objects:  59% (58/98)\u001b[K\rremote: Counting objects:  60% (59/98)\u001b[K\rremote: Counting objects:  61% (60/98)\u001b[K\rremote: Counting objects:  62% (61/98)\u001b[K\rremote: Counting objects:  63% (62/98)\u001b[K\rremote: Counting objects:  64% (63/98)\u001b[K\rremote: Counting objects:  65% (64/98)\u001b[K\rremote: Counting objects:  66% (65/98)\u001b[K\rremote: Counting objects:  67% (66/98)\u001b[K\rremote: Counting objects:  68% (67/98)\u001b[K\rremote: Counting objects:  69% (68/98)\u001b[K\rremote: Counting objects:  70% (69/98)\u001b[K\rremote: Counting objects:  71% (70/98)\u001b[K\rremote: Counting objects:  72% (71/98)\u001b[K\rremote: Counting objects:  73% (72/98)\u001b[K\rremote: Counting objects:  74% (73/98)\u001b[K\rremote: Counting objects:  75% (74/98)\u001b[K\rremote: Counting objects:  76% (75/98)\u001b[K\rremote: Counting objects:  77% (76/98)\u001b[K\rremote: Counting objects:  78% (77/98)\u001b[K\rremote: Counting objects:  79% (78/98)\u001b[K\rremote: Counting objects:  80% (79/98)\u001b[K\rremote: Counting objects:  81% (80/98)\u001b[K\rremote: Counting objects:  82% (81/98)\u001b[K\rremote: Counting objects:  83% (82/98)\u001b[K\rremote: Counting objects:  84% (83/98)\u001b[K\rremote: Counting objects:  85% (84/98)\u001b[K\rremote: Counting objects:  86% (85/98)\u001b[K\rremote: Counting objects:  87% (86/98)\u001b[K\rremote: Counting objects:  88% (87/98)\u001b[K\rremote: Counting objects:  89% (88/98)\u001b[K\rremote: Counting objects:  90% (89/98)\u001b[K\rremote: Counting objects:  91% (90/98)\u001b[K\rremote: Counting objects:  92% (91/98)\u001b[K\rremote: Counting objects:  93% (92/98)\u001b[K\rremote: Counting objects:  94% (93/98)\u001b[K\rremote: Counting objects:  95% (94/98)\u001b[K\rremote: Counting objects:  96% (95/98)\u001b[K\rremote: Counting objects:  97% (96/98)\u001b[K\rremote: Counting objects:  98% (97/98)\u001b[K\rremote: Counting objects: 100% (98/98)\u001b[K\rremote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/83)\u001b[K\rremote: Compressing objects:   2% (2/83)\u001b[K\rremote: Compressing objects:   3% (3/83)\u001b[K\rremote: Compressing objects:   4% (4/83)\u001b[K\rremote: Compressing objects:   6% (5/83)\u001b[K\rremote: Compressing objects:   7% (6/83)\u001b[K\rremote: Compressing objects:   8% (7/83)\u001b[K\rremote: Compressing objects:   9% (8/83)\u001b[K\rremote: Compressing objects:  10% (9/83)\u001b[K\rremote: Compressing objects:  12% (10/83)\u001b[K\rremote: Compressing objects:  13% (11/83)\u001b[K\rremote: Compressing objects:  14% (12/83)\u001b[K\rremote: Compressing objects:  15% (13/83)\u001b[K\rremote: Compressing objects:  16% (14/83)\u001b[K\rremote: Compressing objects:  18% (15/83)\u001b[K\rremote: Compressing objects:  19% (16/83)\u001b[K\rremote: Compressing objects:  20% (17/83)\u001b[K\rremote: Compressing objects:  21% (18/83)\u001b[K\rremote: Compressing objects:  22% (19/83)\u001b[K\rremote: Compressing objects:  24% (20/83)\u001b[K\rremote: Compressing objects:  25% (21/83)\u001b[K\rremote: Compressing objects:  26% (22/83)\u001b[K\rremote: Compressing objects:  27% (23/83)\u001b[K\rremote: Compressing objects:  28% (24/83)\u001b[K\rremote: Compressing objects:  30% (25/83)\u001b[K\rremote: Compressing objects:  31% (26/83)\u001b[K\rremote: Compressing objects:  32% (27/83)\u001b[K\rremote: Compressing objects:  33% (28/83)\u001b[K\rremote: Compressing objects:  34% (29/83)\u001b[K\rremote: Compressing objects:  36% (30/83)\u001b[K\rremote: Compressing objects:  37% (31/83)\u001b[K\rremote: Compressing objects:  38% (32/83)\u001b[K\rremote: Compressing objects:  39% (33/83)\u001b[K\rremote: Compressing objects:  40% (34/83)\u001b[K\rremote: Compressing objects:  42% (35/83)\u001b[K\rremote: Compressing objects:  43% (36/83)\u001b[K\rremote: Compressing objects:  44% (37/83)\u001b[K\rremote: Compressing objects:  45% (38/83)\u001b[K\rremote: Compressing objects:  46% (39/83)\u001b[K\rremote: Compressing objects:  48% (40/83)\u001b[K\rremote: Compressing objects:  49% (41/83)\u001b[K\rremote: Compressing objects:  50% (42/83)\u001b[K\rremote: Compressing objects:  51% (43/83)\u001b[K\rremote: Compressing objects:  53% (44/83)\u001b[K\rremote: Compressing objects:  54% (45/83)\u001b[K\rremote: Compressing objects:  55% (46/83)\u001b[K\rremote: Compressing objects:  56% (47/83)\u001b[K\rremote: Compressing objects:  57% (48/83)\u001b[K\rremote: Compressing objects:  59% (49/83)\u001b[K\rremote: Compressing objects:  60% (50/83)\u001b[K\rremote: Compressing objects:  61% (51/83)\u001b[K\rremote: Compressing objects:  62% (52/83)\u001b[K\rremote: Compressing objects:  63% (53/83)\u001b[K\rremote: Compressing objects:  65% (54/83)\u001b[K\rremote: Compressing objects:  66% (55/83)\u001b[K\rremote: Compressing objects:  67% (56/83)\u001b[K\rremote: Compressing objects:  68% (57/83)\u001b[K\rremote: Compressing objects:  69% (58/83)\u001b[K\rremote: Compressing objects:  71% (59/83)\u001b[K\rremote: Compressing objects:  72% (60/83)\u001b[K\rremote: Compressing objects:  73% (61/83)\u001b[K\rremote: Compressing objects:  74% (62/83)\u001b[K\rremote: Compressing objects:  75% (63/83)\u001b[K\rremote: Compressing objects:  77% (64/83)\u001b[K\rremote: Compressing objects:  78% (65/83)\u001b[K\rremote: Compressing objects:  79% (66/83)\u001b[K\rremote: Compressing objects:  80% (67/83)\u001b[K\rremote: Compressing objects:  81% (68/83)\u001b[K\rremote: Compressing objects:  83% (69/83)\u001b[K\rremote: Compressing objects:  84% (70/83)\u001b[K\rremote: Compressing objects:  85% (71/83)\u001b[K\rremote: Compressing objects:  86% (72/83)\u001b[K\rremote: Compressing objects:  87% (73/83)\u001b[K\rremote: Compressing objects:  89% (74/83)\u001b[K\rremote: Compressing objects:  90% (75/83)\u001b[K\rremote: Compressing objects:  91% (76/83)\u001b[K\rremote: Compressing objects:  92% (77/83)\u001b[K\rremote: Compressing objects:  93% (78/83)\u001b[K\rremote: Compressing objects:  95% (79/83)\u001b[K\rremote: Compressing objects:  96% (80/83)\u001b[K\rremote: Compressing objects:  97% (81/83)\u001b[K\rremote: Compressing objects:  98% (82/83)\u001b[K\rremote: Compressing objects: 100% (83/83)\u001b[K\rremote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "Receiving objects:   0% (1/716)   \rReceiving objects:   1% (8/716)   \rReceiving objects:   2% (15/716)   \rReceiving objects:   3% (22/716)   \rReceiving objects:   4% (29/716)   \rReceiving objects:   5% (36/716)   \rReceiving objects:   6% (43/716)   \rReceiving objects:   7% (51/716)   \rReceiving objects:   8% (58/716)   \rReceiving objects:   9% (65/716)   \rReceiving objects:  10% (72/716)   \rReceiving objects:  11% (79/716)   \rReceiving objects:  12% (86/716)   \rReceiving objects:  13% (94/716)   \rReceiving objects:  14% (101/716)   \rReceiving objects:  15% (108/716)   \rReceiving objects:  16% (115/716)   \rReceiving objects:  17% (122/716)   \rReceiving objects:  18% (129/716)   \rReceiving objects:  19% (137/716)   \rReceiving objects:  20% (144/716)   \rReceiving objects:  21% (151/716)   \rReceiving objects:  22% (158/716)   \rReceiving objects:  23% (165/716)   \rReceiving objects:  24% (172/716)   \rReceiving objects:  25% (179/716)   \rReceiving objects:  26% (187/716)   \rReceiving objects:  27% (194/716)   \rReceiving objects:  28% (201/716)   \rReceiving objects:  29% (208/716)   \rReceiving objects:  30% (215/716)   \rReceiving objects:  31% (222/716)   \rReceiving objects:  32% (230/716)   \rReceiving objects:  33% (237/716)   \rReceiving objects:  34% (244/716)   \rReceiving objects:  35% (251/716)   \rReceiving objects:  36% (258/716)   \rReceiving objects:  37% (265/716)   \rReceiving objects:  38% (273/716)   \rReceiving objects:  39% (280/716)   \rReceiving objects:  40% (287/716)   \rReceiving objects:  41% (294/716)   \rReceiving objects:  42% (301/716)   \rReceiving objects:  43% (308/716)   \rReceiving objects:  44% (316/716)   \rReceiving objects:  45% (323/716)   \rReceiving objects:  46% (330/716)   \rReceiving objects:  47% (337/716)   \rReceiving objects:  48% (344/716)   \rReceiving objects:  49% (351/716)   \rReceiving objects:  50% (358/716)   \rReceiving objects:  51% (366/716)   \rReceiving objects:  52% (373/716)   \rReceiving objects:  53% (380/716)   \rReceiving objects:  54% (387/716)   \rReceiving objects:  55% (394/716)   \rReceiving objects:  56% (401/716)   \rReceiving objects:  57% (409/716)   \rReceiving objects:  58% (416/716)   \rReceiving objects:  59% (423/716)   \rReceiving objects:  60% (430/716)   \rReceiving objects:  61% (437/716)   \rReceiving objects:  62% (444/716)   \rReceiving objects:  63% (452/716)   \rReceiving objects:  64% (459/716)   \rReceiving objects:  65% (466/716)   \rReceiving objects:  66% (473/716)   \rReceiving objects:  67% (480/716)   \rReceiving objects:  68% (487/716)   \rReceiving objects:  69% (495/716)   \rReceiving objects:  70% (502/716)   \rReceiving objects:  71% (509/716)   \rReceiving objects:  72% (516/716)   \rReceiving objects:  73% (523/716)   \rReceiving objects:  74% (530/716)   \rReceiving objects:  75% (537/716)   \rReceiving objects:  76% (545/716)   \rReceiving objects:  77% (552/716)   \rReceiving objects:  78% (559/716)   \rReceiving objects:  79% (566/716)   \rReceiving objects:  80% (573/716)   \rReceiving objects:  81% (580/716)   \rReceiving objects:  82% (588/716)   \rReceiving objects:  83% (595/716)   \rReceiving objects:  84% (602/716)   \rremote: Total 716 (delta 38), reused 44 (delta 15), pack-reused 618\u001b[K\n",
            "Receiving objects:  85% (609/716)   \rReceiving objects:  86% (616/716)   \rReceiving objects:  87% (623/716)   \rReceiving objects:  88% (631/716)   \rReceiving objects:  89% (638/716)   \rReceiving objects:  90% (645/716)   \rReceiving objects:  91% (652/716)   \rReceiving objects:  92% (659/716)   \rReceiving objects:  93% (666/716)   \rReceiving objects:  94% (674/716)   \rReceiving objects:  95% (681/716)   \rReceiving objects:  96% (688/716)   \rReceiving objects:  97% (695/716)   \rReceiving objects:  98% (702/716)   \rReceiving objects:  99% (709/716)   \rReceiving objects: 100% (716/716)   \rReceiving objects: 100% (716/716), 14.19 MiB | 36.88 MiB/s, done.\n",
            "Resolving deltas:   0% (0/421)   \rResolving deltas:   1% (7/421)   \rResolving deltas:   4% (17/421)   \rResolving deltas:   5% (24/421)   \rResolving deltas:   7% (31/421)   \rResolving deltas:   9% (42/421)   \rResolving deltas:  17% (73/421)   \rResolving deltas:  23% (97/421)   \rResolving deltas:  24% (105/421)   \rResolving deltas:  27% (115/421)   \rResolving deltas:  30% (130/421)   \rResolving deltas:  32% (135/421)   \rResolving deltas:  34% (145/421)   \rResolving deltas:  35% (148/421)   \rResolving deltas:  36% (152/421)   \rResolving deltas:  37% (159/421)   \rResolving deltas:  38% (161/421)   \rResolving deltas:  41% (173/421)   \rResolving deltas:  42% (178/421)   \rResolving deltas:  43% (183/421)   \rResolving deltas:  44% (186/421)   \rResolving deltas:  45% (190/421)   \rResolving deltas:  46% (197/421)   \rResolving deltas:  47% (198/421)   \rResolving deltas:  48% (205/421)   \rResolving deltas:  49% (208/421)   \rResolving deltas:  50% (213/421)   \rResolving deltas:  51% (215/421)   \rResolving deltas:  52% (220/421)   \rResolving deltas:  53% (225/421)   \rResolving deltas:  54% (230/421)   \rResolving deltas:  56% (238/421)   \rResolving deltas:  57% (241/421)   \rResolving deltas:  58% (247/421)   \rResolving deltas:  59% (249/421)   \rResolving deltas:  60% (253/421)   \rResolving deltas:  61% (258/421)   \rResolving deltas:  62% (262/421)   \rResolving deltas:  63% (266/421)   \rResolving deltas:  65% (275/421)   \rResolving deltas:  68% (287/421)   \rResolving deltas:  69% (291/421)   \rResolving deltas:  82% (349/421)   \rResolving deltas:  83% (351/421)   \rResolving deltas:  84% (356/421)   \rResolving deltas:  85% (361/421)   \rResolving deltas:  86% (364/421)   \rResolving deltas:  87% (367/421)   \rResolving deltas:  89% (377/421)   \rResolving deltas:  90% (382/421)   \rResolving deltas:  91% (384/421)   \rResolving deltas:  92% (390/421)   \rResolving deltas:  93% (393/421)   \rResolving deltas:  94% (396/421)   \rResolving deltas:  95% (402/421)   \rResolving deltas:  96% (405/421)   \rResolving deltas:  99% (420/421)   \rResolving deltas: 100% (421/421)   \rResolving deltas: 100% (421/421), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm9cXH6ExtJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -U tensorflow_estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HF8-OxXVjd",
        "colab_type": "code",
        "outputId": "f29d54a0-6c92-44d3-af31-078e232af8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LDNDjxnUiym",
        "colab_type": "code",
        "outputId": "10df0cad-1111-438a-f89f-53f6e03c2185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "# import python and waymo dataset libraries \n",
        "\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "# fixes import error for python 2.7\n",
        "sys.path.insert(1, r\"/usr/local/lib/python3.6/dist-packages\")\n",
        "\n",
        "!pip install waymo-open-dataset\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import inspect\n",
        "import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import itertools\n",
        "import imghdr\n",
        "import functools\n",
        "import math\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import collections\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "from waymo_open_dataset.utils import range_image_utils\n",
        "from waymo_open_dataset.utils import transform_utils\n",
        "from waymo_open_dataset.utils import  frame_utils\n",
        "from waymo_open_dataset.utils import  box_utils\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
        "\n",
        "# This cell requires to restart colab run-time after exec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: waymo-open-dataset in /usr/local/lib/python2.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: tensorflow>=1.14.0 in /tensorflow-1.15.2/python2.7 (from waymo-open-dataset) (1.15.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.16.4)\n",
            "Requirement already satisfied: wheel; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python2.7 (from tensorflow>=1.14.0->waymo-open-dataset) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (3.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.1.0)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.0.post1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.15.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.0.8)\n",
            "Requirement already satisfied: functools32>=3.2.3; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (3.2.3.post2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.11.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.7.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (2.3.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.1.7)\n",
            "Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.1.6)\n",
            "Requirement already satisfied: mock>=2.0.0; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (2.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow>=1.14.0->waymo-open-dataset) (3.2.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14.0->waymo-open-dataset) (44.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14.0->waymo-open-dataset) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14.0->waymo-open-dataset) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14.0->waymo-open-dataset) (2.8.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow>=1.14.0->waymo-open-dataset) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow>=1.14.0->waymo-open-dataset) (5.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRo-nNL7VAYQ",
        "colab_type": "text"
      },
      "source": [
        "### Waymo-OD to Kitti adapter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbMCwXPwZ38P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform Waymo dataset style to KITTI dataset style\n",
        "# slightly modified version of https://github.com/Yao-Shao/Waymo_Kitti_Adapter conversion program "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pboh9CIaVFB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ./drive/'My Drive'/aid-submission/adapter_file_v3.py --source_dir ./drive/'My Drive'/Waymo_OD --target_dir ./drive/'My Drive'/Waymo_to_Kitti_adapter_train_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6RYsrQhUxRH",
        "colab_type": "text"
      },
      "source": [
        "### Setup frustum Point-net (YOLOv3 backbone) model for Waymo-OD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cReEujaTaZ7g",
        "colab_type": "text"
      },
      "source": [
        "##### Setup YOLOv3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8ET9uSUay0F",
        "colab_type": "code",
        "outputId": "99da4fff-e97b-4cc2-f348-b6acc26f8ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!cat ./drive/'My Drive'/aid-submission/setup.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/bin/bash\n",
            "\n",
            "git clone https://github.com/wizyoung/YOLOv3_TensorFlow.git ./drive/'My Drive'/YOLOv3_TensorFlow/\n",
            "wget https://pjreddie.com/media/files/yolov3.weights -P ./drive/'My Drive'/YOLOv3_TensorFlow/data/darknet_weights/\n",
            "echo \"\" > ./drive/'My Drive'/YOLOv3_TensorFlow/__init__.py\n",
            "cd ./drive/'My Drive'/YOLOv3_TensorFlow/\n",
            "python convert_weight.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3GMIuF9U9xU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download pretrained on ImageNet YOLOv3\n",
        "\n",
        "!bash ./drive/'My Drive'/aid-submission/setup.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSAY3W8uaYyB",
        "colab_type": "text"
      },
      "source": [
        "##### Setup frustum pointnet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1R1icHobutI",
        "colab_type": "code",
        "outputId": "b46beb4e-49f0-4c9f-fe7b-dfe5eddb956e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Modules path managing\n",
        "\n",
        "!pip install opencv-python\n",
        "!cp -r ./drive/'My Drive'/YOLOv3_TensorFlow ./\n",
        "\n",
        "sys.path.insert(1, \"/usr/local/lib/python2.7/dist-packages/cv2\")\n",
        "sys.path.insert(1, \"/content/drive/My Drive/\")\n",
        "sys.path.insert(1, r\"./drive/My Drive/aid-submission\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python2.7/dist-packages (3.4.5.20)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from opencv-python) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAJlT6cwaksj",
        "colab_type": "code",
        "outputId": "09cbb063-9ffb-4c3f-e3e7-000b40503d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# delete cached .pyc files for editing purposes\n",
        "# !find ./drive/'My Drive'/aid-submission/ -name '*.pyc' -delete \n",
        "\n",
        "from collections import defaultdict\n",
        "from matplotlib import cm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from data_loader import Parser\n",
        "from detection2d import Detector2D\n",
        "from detection3d import Detector3D\n",
        "\n",
        "import messages\n",
        "from messages import Detection2d, Detection3d\n",
        "from transformer import Transformer\n",
        "from frustums import single_frustum_iterator\n",
        "from training.pointnet import find_person_position"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0330 21:45:40.153280 140426287335296 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRjy5ksstiYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class frustum_FP_sampler(object):\n",
        "  \"\"\"Samples FP pedestrian detections via frustum pointnet\"\"\"\n",
        "\n",
        "  def __init__(self, model_path, dataset_path, labled_mode_flag):\n",
        "    \"\"\"Initialize folder path and setup input parser.\"\"\"\n",
        "    self.dataset_path = dataset_path\n",
        "    self.labled_mode_flag = labled_mode_flag\n",
        "    self.parser = Parser(dataset_path, labled_mode_flag)\n",
        "    self.fp_pedestrian = []\n",
        "\n",
        "    self.GT_count = 0\n",
        "    \n",
        "    self.detector_2d = Detector2D()\n",
        "    self.detector_3d = Detector3D(model_path)\n",
        "\n",
        "    self.cmap = plt.get_cmap('jet')\n",
        "    \n",
        "\n",
        "  def run_detector(self, num_frames, camera_id=2, plot_det=False, fp_min_pts_num_threshold=75, fp_intersection_ratio=1./2., dest_path=None):\n",
        "    \"\"\"\n",
        "    Performs FP detections on data from parser.\n",
        "    Arguments:\n",
        "      num_frames (int): set equals to -1 for full dataset parsing\n",
        "\n",
        "      camera_id (int from 0 to 4): what camera should be used for 2D YOLO detection\n",
        "\n",
        "      plot_det (bool): visualize detection (image + 2D-bbox + lidar points segmented as pedestrians)\n",
        "\n",
        "      fp_min_pts_num_threshold (int): minimal number of points in frustum (that are classified as pedestrians) to consider it a valid classification\n",
        "      (e.g. if there are only 10 points segmented as pedestrian its not a valid detection)\n",
        "\n",
        "      fp_intersection_ratio (float from 0 to 1): minimal fraction of correctly segmented points in frustum to consider it a TP detection\n",
        "\n",
        "      dest_path (NoneType or string): Where false positive detection must be saved as .npy binary files\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(num_frames) == int\n",
        "    assert camera_id in {0,1,2,3,4}\n",
        "    assert type(plot_det) == bool\n",
        "    assert type(fp_min_pts_num_threshold) == int\n",
        "    assert type(fp_intersection_ratio) == float and (0 < fp_intersection_ratio < 1)\n",
        "    assert dest_path is None or type(dest_path) == str\n",
        "\n",
        "    max_frame_num = float('inf') if num_frames == -1 else num_frames\n",
        "    \n",
        "    if dest_path is not None and not os.path.exists(dest_path):\n",
        "      os.mkdir(dest_path)\n",
        "\n",
        "    self.current_iter_id = 0 \n",
        "    self.frame_number = 0\n",
        "    \n",
        "    # DEBUG:\n",
        "    # self.test_frustum_points = []\n",
        "\n",
        "    with self.detector_2d:\n",
        "      for image, velo, calib, labels_ in self._labled_parser(self.parser):\n",
        "        print(\"Frame number: {0}\".format(self.frame_number))\n",
        "      \n",
        "        GT_pedestrians_box_3D, GT_pedestrians_box_2D = self._parse_labels_to_boxes(labels_)\n",
        "\n",
        "        self.GT_count += len(GT_pedestrians_box_3D)\n",
        "\n",
        "        boxes, scores, labels = self.detector_2d(image[:, :, :3])\n",
        "\n",
        "        detections_2d = self._convert_yolo_2_detections(boxes, scores, labels)\n",
        "\n",
        "        # REMIDER: update velo (it will have NLZ tags as 4-th column in it) \n",
        "        detections_3d = self.detector_3d(image, velo, calib, detections_2d)\n",
        "        \n",
        "        transformer = Transformer(calib)\n",
        "\n",
        "        if plot_det: # render GT bboxes and YOLO bboxes\n",
        "          projected_points, mask = transformer.project_points(velo[:, :3], image.shape,\n",
        "                                                              return_only_valid=False)\n",
        "          for gt in GT_pedestrians_box_2D:\n",
        "            cv2.rectangle(image, (int(gt[0,0]), int(gt[1,1])), (int(gt[1,0]), int(gt[0,1])), (0, 0, 255, 255), 3)\n",
        "\n",
        "          for d in detections_2d:\n",
        "            cv2.rectangle(image, (int(d.x_min), int(d.y_min)), (int(d.x_max), int(d.y_max)), (0, 255, 0, 255), 3)\n",
        "        \n",
        "\n",
        "        vtc_tr_matrix = calib[\"Tr_velo_to_cam_{0}\".format(camera_id)].reshape(4,4) # Get calibration matrix for required camera \n",
        "        inv_vtc_transform_matrix = np.linalg.inv(vtc_tr_matrix)[:3, :3].T          # Inverse matrix (project from sensor frame to vehicle frame) \n",
        "\n",
        "\n",
        "        for frustum_points, frustum_mask in single_frustum_iterator(image, velo, transformer, detections_2d):\n",
        "\n",
        "            point_labels_mask = (self.detector_3d.pedestrian_model.predict([frustum_points, \n",
        "                                                                        np.zeros(frustum_points.shape[0], dtype=np.int32)])[0] > 0)\n",
        "\n",
        "            masked_points = frustum_points[point_labels_mask]\n",
        "            # Transform lidar points to vehicle frame\n",
        "            frustum_points_vc = np.dot(masked_points - vtc_tr_matrix[:3, 3], inv_vtc_transform_matrix)\n",
        "            \n",
        "            \n",
        "            if len(masked_points) < fp_min_pts_num_threshold: # not enough segmented points in frustum\n",
        "              print(\"Not enough points in frustum. Skipped.\")\n",
        "              continue\n",
        "            \n",
        "            print(self.current_iter_id, len(GT_pedestrians_box_3D))\n",
        "            if len(GT_pedestrians_box_3D) > 0:\n",
        "              max_intersection_pts = np.max(\n",
        "                                            box_utils.compute_num_points_in_box_3d(\n",
        "                                                          tf.constant(frustum_points_vc), \n",
        "                                                          tf.constant(GT_pedestrians_box_3D)).eval(session=self.detector_2d.session)\n",
        "                                            )\n",
        "              # True positive detection (=> skip to next detection)\n",
        "              if max_intersection_pts > fp_intersection_ratio * len(masked_points):\n",
        "                print(\"True positive detection.\")\n",
        "                continue              \n",
        "\n",
        "            #self.test_frustum_points.append(frustum_points_vc)\n",
        "\n",
        "            print(\"FP detections of size {0} created: {1}/{2}\".format(len(frustum_points_vc), self.current_iter_id + 1, max_frame_num))\n",
        "\n",
        "            self.current_iter_id += 1\n",
        "            if dest_path:\n",
        "              np.save(os.path.join(dest_path, \"{0}.npy\".format(self.current_iter_id).zfill(12)), frustum_points_vc)\n",
        "\n",
        "\n",
        "            # segmented points variation\n",
        "            if plot_det:             \n",
        "              for p in projected_points[frustum_mask][point_labels_mask]:\n",
        "                color = tuple(int(255 * x) for x in self.cmap(p[2] / 40.0)[:3])\n",
        "                cv2.circle(image, (int(p[0]), int(p[1])), 4, color, -1)\n",
        "\n",
        "        if plot_det:\n",
        "          pprint(labels_)\n",
        "          cv2_imshow(cv2.resize(image, (820, 820)))\n",
        "        \n",
        "        self.frame_number += 1\n",
        "\n",
        "        if max_frame_num <= self.current_iter_id:\n",
        "            break\n",
        "\n",
        "\n",
        "  def get_fp_point_cloud_size(self):\n",
        "    return list(map(len,  self.fp_pedestrian))\n",
        "\n",
        "\n",
        "  def _parse_labels_to_boxes(self, labels):\n",
        "    boxes_2D = []\n",
        "    boxes_3D = []\n",
        "\n",
        "    for gt in labels:\n",
        "      if gt['type'] == \"PEDESTRIAN\":\n",
        "        boxes_2D.append(gt['bbox'].reshape(2,2))\n",
        "        boxes_3D.append(np.hstack([gt['location'], np.flip(gt['dimensions']), gt['rotation_y']]))\n",
        "\n",
        "    return np.array(boxes_3D), np.array(boxes_2D)\n",
        "\n",
        "\n",
        "  def _labled_parser(self, parser):\n",
        "    if self.labled_mode_flag:\n",
        "        for image, velo, calib, labels in parser.all:\n",
        "            yield image, velo, calib, labels\n",
        "    else:\n",
        "        for image, velo, calib in parser.all:\n",
        "            yield image, velo, calib, []\n",
        "\n",
        "\n",
        "  def _convert_yolo_2_detections(self, boxes, scores, labels):\n",
        "    detections_2d = []\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "        if label != 0:  # person\n",
        "            continue\n",
        "        d = Detection2d()\n",
        "        d.x_min = box[0]\n",
        "        d.y_min = box[1]\n",
        "        d.x_max = box[2]\n",
        "        d.y_max = box[3]\n",
        "        d.label = messages.pedestrian\n",
        "        d.score = score\n",
        "        detections_2d.append(d)\n",
        "    return detections_2d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSpFMoNSAqTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = \"./drive/My Drive/aid-submission/models/pedestrian\"\n",
        "WAYMO_PATH = \"./drive/My Drive/Waymo_to_Kitti_adapter_train_1/\"\n",
        "LABLED_MODE_FLAG = True\n",
        "\n",
        "FP_sampler = frustum_FP_sampler(MODEL_PATH, WAYMO_PATH, LABLED_MODE_FLAG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om7MDWnQyLxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fill_with_empty_lables(waymo_path, label_folder, image_folder):\n",
        "  \"\"\"Fills label_folder files with empty label .txt files for frustum-net name consistency with image_folder .png files\"\"\"\n",
        "  existing_label_names = set(map(lambda fn: os.path.splitext(fn)[0], \n",
        "                             os.listdir(os.path.join(waymo_path, label_folder))))\n",
        "  \n",
        "  required_names = set(map(lambda fn: os.path.splitext(fn)[0], \n",
        "                           os.listdir(os.path.join(waymo_path, image_folder))))\n",
        "  \n",
        "  for empty_labels in (required_names - existing_label_names):\n",
        "    open(os.path.join(waymo_path, label_folder, empty_labels + \".txt\"), \"a\").close()\n",
        "    \n",
        "fill_with_empty_lables(WAYMO_PATH, \"label_2\", \"image_2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir0bUa__6hF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_pngs(path_to_img=\"/content/drive/My Drive/Waymo_to_Kitti_adapter_train_1/image_2\"):\n",
        "    return all([imghdr.what(os.path.join(path_to_img, img)) == \"png\" for img in os.listdir(path_to_img)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyIvVuPV_vyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FP_sampler.run_detector(1000, plot_det=False, fp_min_pts_num_threshold=30, dest_path=\"/content/drive/My Drive/PointNet/Data_folder_train_0001/TYPE_FP_PEDESTRIAN\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcwkGq5TptzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Once crashed with: TypeError: long() argument must be a string or a number, not 'PngImageFile'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}