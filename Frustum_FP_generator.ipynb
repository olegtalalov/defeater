{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frustum_FP_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ1NM-U98_LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# before running this cell one should setup YOLO via setup.sh\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import zipfile\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "from waymo_open_dataset.utils import range_image_utils\n",
        "from waymo_open_dataset.utils import transform_utils\n",
        "from waymo_open_dataset.utils import  frame_utils\n",
        "from waymo_open_dataset.utils import  box_utils\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
        "\n",
        "from data_loader import Parser\n",
        "from detection2d import Detector2D\n",
        "from detection3d import Detector3D\n",
        "\n",
        "import messages\n",
        "from messages import Detection2d, Detection3d\n",
        "from transformer import Transformer\n",
        "from frustums import single_frustum_iterator\n",
        "from training.pointnet import find_person_position"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dggDEvx9TQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class frustum_FP_sampler(object):\n",
        "  \"\"\"Samples FP pedestrian detections via frustum pointnet\"\"\"\n",
        "\n",
        "  def __init__(self, model_path, dataset_path, labled_mode_flag):\n",
        "    \"\"\"Initialize folder path and setup input parser.\"\"\"\n",
        "    self.dataset_path = dataset_path\n",
        "    self.labled_mode_flag = labled_mode_flag\n",
        "    self.fp_pedestrian = []\n",
        "\n",
        "    self.GT_count = 0\n",
        "    self.detector_2d = Detector2D()\n",
        "    self.detector_3d = Detector3D(model_path)\n",
        "\n",
        "    self.cmap = plt.get_cmap('jet')\n",
        "    \n",
        "\n",
        "  def run_detector(self, num_frames, plot_det=False, \n",
        "                   fp_min_pts_num_threshold=75, fp_intersection_ratio=1./2., dest_path=None, camera_id=2):\n",
        "    \"\"\"\n",
        "    Performs FP detections on data from parser.\n",
        "    Arguments:\n",
        "      num_frames (int): set equals to -1 for full dataset parsing\n",
        "\n",
        "      camera_id (int from 0 to 4): what camera should be used for 2D YOLO detection\n",
        "\n",
        "      plot_det (bool): visualize detection (image + 2D-bbox + lidar points segmented as pedestrians)\n",
        "\n",
        "      fp_min_pts_num_threshold (int): minimal number of points in frustum (that are classified as pedestrians) to consider it a valid classification\n",
        "      (e.g. if there are only 10 points segmented as pedestrian its not a valid detection)\n",
        "\n",
        "      fp_intersection_ratio (float from 0 to 1): minimal fraction of correctly segmented points in frustum to consider it a TP detection\n",
        "\n",
        "      dest_path (NoneType or string): Where false positive detection must be saved as .npy binary files\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(num_frames) == int\n",
        "    assert camera_id in {0,1,2,3,4}\n",
        "    assert type(plot_det) == bool\n",
        "    assert type(fp_min_pts_num_threshold) == int\n",
        "    assert type(fp_intersection_ratio) == float and (0 < fp_intersection_ratio < 1)\n",
        "    assert dest_path is None or type(dest_path) == str\n",
        "\n",
        "    self.parser = Parser(self.dataset_path, self.labled_mode_flag, camera_id)\n",
        "\n",
        "    max_frame_num = float('inf') if num_frames == -1 else num_frames\n",
        "    \n",
        "    if dest_path is not None and not os.path.exists(dest_path):\n",
        "      os.mkdir(dest_path)\n",
        "\n",
        "    self.current_iter_id = 0 \n",
        "    self.frame_number = 0\n",
        "    \n",
        "    # DEBUG:\n",
        "    self.test_frustum_points = []\n",
        "    self.test_frustum_points_2 = []\n",
        "\n",
        "    with self.detector_2d:\n",
        "      for image, velo, calib, labels_ in self._labled_parser(self.parser):\n",
        "        print(\"Frame number: {0}\".format(self.frame_number))\n",
        "      \n",
        "        GT_pedestrians_box_3D, GT_pedestrians_box_2D = self._parse_labels_to_boxes(labels_)\n",
        "\n",
        "        self.GT_count += len(GT_pedestrians_box_3D)\n",
        "\n",
        "        velo = velo[:, :4][velo[:, -1] == -1] # only labeled zones\n",
        "\n",
        "        boxes, scores, labels = self.detector_2d(image[:, :, :3])\n",
        "\n",
        "        detections_2d = self._convert_yolo_2_detections(boxes, scores, labels)\n",
        "     \n",
        "        pts_detections_3d = self.detector_3d(image, velo, calib, detections_2d, camera_id, shift_pos=False)\n",
        "\n",
        "        transformer = Transformer(calib, camera_id)\n",
        "\n",
        "        if plot_det: # render GT bboxes and YOLO bboxes\n",
        "          for gt in GT_pedestrians_box_2D:\n",
        "            cv2.rectangle(image, (int(gt[0,0]), int(gt[1,1])), (int(gt[1,0]), int(gt[0,1])), (0, 0, 255, 255), 3)\n",
        "\n",
        "          for d in detections_2d:\n",
        "            cv2.rectangle(image, (int(d.x_min), int(d.y_min)), (int(d.x_max), int(d.y_max)), (0, 255, 0, 255), 3)\n",
        "        \n",
        "        axis_shuffle_matr = np.array([[0,-1,0,0], [0,0,-1,0], [1,0,0,0], [0,0,0,1]])\n",
        "        vtc_tr_matrix = np.dot(axis_shuffle_matr, calib[\"Tr_velo_to_cam_{0}\".format(camera_id)].reshape(4,4)) # Get calibration matrix for required camera\n",
        "        \n",
        "        # Inverse matrix (project from sensor frame to vehicle frame)  \n",
        "        inv_vtc_transform_matrix = np.linalg.inv(vtc_tr_matrix)[:3, :3].T         \n",
        "\n",
        "        print(\"NUMBER OF DETECTIONS: {0}\".format(len(detections_2d)))\n",
        "\n",
        "        for det3d in pts_detections_3d:\n",
        "          if len(det3d) == 0:\n",
        "            continue\n",
        "\n",
        "          person_pts_vc = np.dot(det3d - vtc_tr_matrix[:3, 3], inv_vtc_transform_matrix)\n",
        "          \n",
        "          self.test_frustum_points_2.append((np.isclose(velo[:, :3][:, None], person_pts_vc)).all(-1).argmax(0))\n",
        "\n",
        "          if len(person_pts_vc) < fp_min_pts_num_threshold: # not enough segmented points in frustum\n",
        "            print(\"Not enough points in frustum. Skipped.\")\n",
        "            continue\n",
        "\n",
        "          print(self.current_iter_id, len(GT_pedestrians_box_3D))\n",
        "\n",
        "          if len(GT_pedestrians_box_3D) > 0:\n",
        "            max_intersection_pts = np.max(\n",
        "                                          box_utils.compute_num_points_in_box_3d(\n",
        "                                                        tf.constant(person_pts_vc), \n",
        "                                                        tf.constant(GT_pedestrians_box_3D)).eval(session=self.detector_2d.session)\n",
        "                                          )\n",
        "            \n",
        "          # True positive detection (=> skip to next detection)\n",
        "          if max_intersection_pts > fp_intersection_ratio * len(person_pts_vc):\n",
        "            print(\"True positive detection.\")\n",
        "            continue              \n",
        "\n",
        "          print(\"FP detections of size {0} created: {1}/{2}\".format(len(person_pts_vc), self.current_iter_id + 1, max_frame_num))\n",
        "\n",
        "          self.current_iter_id += 1\n",
        "          if dest_path:\n",
        "            np.save(os.path.join(dest_path, \"{0}.npy\".format(self.current_iter_id).zfill(12)), person_pts_vc)\n",
        "\n",
        "          self.test_frustum_points.append(person_pts_vc)\n",
        "          # segmented points variation\n",
        "          if plot_det:\n",
        "            projected_points, mask = transformer.project_points(person_pts_vc[:, :3], image.shape,\n",
        "                                                                return_only_valid=True)\n",
        "            for p in projected_points:\n",
        "              color = tuple(int(255 * x) for x in self.cmap(p[2] / 40.0)[:3])\n",
        "              cv2.circle(image, (int(p[0]), int(p[1])), 10, [255, 0, 0], -1)\n",
        "\n",
        "        if plot_det:\n",
        "          pprint(\"GT_pedestrians: {0}\".format(len(GT_pedestrians_box_3D))) \n",
        "          cv2.imshow(cv2.resize(image, (820, 820)))\n",
        "        \n",
        "        self.frame_number += 1\n",
        "\n",
        "        if max_frame_num <= self.current_iter_id:\n",
        "            break\n",
        "\n",
        "\n",
        "  def get_fp_point_cloud_size(self):\n",
        "    return list(map(len,  self.fp_pedestrian))\n",
        "\n",
        "\n",
        "  def _parse_labels_to_boxes(self, labels, obj_type=[\"PEDESTRIAN\", \"CYCLIST\", \"VEHICLE\"]):\n",
        "    boxes_2D = []\n",
        "    boxes_3D = []\n",
        "\n",
        "    for gt in labels:\n",
        "      if gt['type'] in obj_type:\n",
        "        boxes_2D.append(gt['bbox'].reshape(2,2))\n",
        "        boxes_3D.append(np.hstack([gt['location'], np.flip(gt['dimensions']), gt['rotation_y']]))\n",
        "\n",
        "    return np.array(boxes_3D), np.array(boxes_2D)\n",
        "\n",
        "\n",
        "  def _labled_parser(self, parser):\n",
        "    if self.labled_mode_flag:\n",
        "        for image, velo, calib, labels in parser.all:\n",
        "            yield image, velo, calib, labels\n",
        "    else:\n",
        "        for image, velo, calib in parser.all:\n",
        "            yield image, velo, calib, []\n",
        "\n",
        "\n",
        "  def _convert_yolo_2_detections(self, boxes, scores, labels):\n",
        "    detections_2d = []\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "        if label != 0:  # person\n",
        "            continue\n",
        "        d = Detection2d()\n",
        "        d.x_min = box[0]\n",
        "        d.y_min = box[1]\n",
        "        d.x_max = box[2]\n",
        "        d.y_max = box[3]\n",
        "        d.label = messages.pedestrian\n",
        "        d.score = score\n",
        "        detections_2d.append(d)\n",
        "    return detections_2d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckIC4iWn9Tc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = \"./aid-submission/models/pedestrian\"\n",
        "WAYMO_PATH = \"./Waymo_to_Kitti_adapter_train_0001/\"\n",
        "LABLED_MODE_FLAG = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCI-TGm5Aqul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FP_sampler = frustum_FP_sampler(MODEL_PATH, WAYMO_PATH, LABLED_MODE_FLAG)\n",
        "\n",
        "for cam_id in range(0, 5):\n",
        "  dest_path = \"/content/FP_detections_camera_{0}/\".format(cam_id)\n",
        "  FP_sampler.run_detector(1600, plot_det=False, fp_min_pts_num_threshold=20, dest_path=dest_path, camera_id=cam_id)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}