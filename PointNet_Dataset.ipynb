{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointNet_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XXqiG6cyh85LbOltOQT4jC-bcdRue8Z1",
      "authorship_tag": "ABX9TyMUSOgSqM7TjPAPhs8ganQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62a1bf53601a46c8b4a817d6b0aeeb32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_59c645f882ab4490974496f6324cdc90",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c914c1ae270410aaae8521f6c3dc4a8",
              "IPY_MODEL_b0b34bfe985a4efe8cba415d0b04054a"
            ]
          }
        },
        "59c645f882ab4490974496f6324cdc90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c914c1ae270410aaae8521f6c3dc4a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_670d688c6ba94279acc92d4a64384998",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 126,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 126,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc93734c5b4f4b809b3d2ea4c306157a"
          }
        },
        "b0b34bfe985a4efe8cba415d0b04054a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_38174a495cd44616b02d76f0073e2935",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 126/126 [22:46&lt;00:00, 10.85s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78820c6067714191b2678d4f0995ba98"
          }
        },
        "670d688c6ba94279acc92d4a64384998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc93734c5b4f4b809b3d2ea4c306157a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38174a495cd44616b02d76f0073e2935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78820c6067714191b2678d4f0995ba98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtyomGrachev/defeater/blob/ArtyomGrachev-PointNet-DeepCosineMetric-v1/PointNet_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxQQSMDJI3W-",
        "colab_type": "text"
      },
      "source": [
        "### Creates pytorch-style Waymo-OD dataset for 3D point classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2kyLYbxlV5s",
        "colab_type": "code",
        "outputId": "cf40100f-a949-41e3-8ebd-4ea09e0686b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!apt-get install tree\n",
        "!pip install importlib\n",
        "!pip install torchvision==0.4.0\n",
        "!pip install  Pillow==6.2.1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 8.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.0.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (539 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 144542 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting importlib\n",
            "  Downloading importlib-1.0.4.zip (7.1 kB)\n",
            "Building wheels for collected packages: importlib\n",
            "  Building wheel for importlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for importlib: filename=importlib-1.0.4-py3-none-any.whl size=5855 sha256=a1c39b1872524c9fe08a1bfc57567204fa805feb806bbabe51f07a6eae801203\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/da/ab/4605932a2873f1c1b9f512d584e3f01664d7e71db2f879962c\n",
            "Successfully built importlib\n",
            "Installing collected packages: importlib\n",
            "Successfully installed importlib-1.0.4\n",
            "Collecting torchvision==0.4.0\n",
            "  Downloading torchvision-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 202 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (1.12.0)\n",
            "Collecting torch==1.2.0\n",
            "  Downloading torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8 MB 13 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.0) (1.18.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.5.0\n",
            "    Uninstalling torchvision-0.5.0:\n",
            "      Successfully uninstalled torchvision-0.5.0\n",
            "Successfully installed torch-1.2.0 torchvision-0.4.0\n",
            "Collecting Pillow==6.2.1\n",
            "  Downloading Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 9.1 MB/s \n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-6.2.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLDapRH6jcoc",
        "colab_type": "code",
        "outputId": "8990beaa-bb45-4ef9-c96b-236919fe6afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "# Clone github Waymo-od repo with util functions\n",
        "\n",
        "!git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-od\n",
        "!pip install waymo-open-dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'waymo-od'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 799 (delta 25), reused 24 (delta 10), pack-reused 749\u001b[K\n",
            "Receiving objects: 100% (799/799), 14.17 MiB | 7.08 MiB/s, done.\n",
            "Resolving deltas: 100% (505/505), done.\n",
            "Collecting waymo-open-dataset\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/49/61ff4a6081233df4b3d7bb1b454a8a9c6a29b493ef84c2b667db4a2b79d5/waymo_open_dataset-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=1.14.0 in /tensorflow-1.15.2/python3.6 (from waymo-open-dataset) (1.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow>=1.14.0->waymo-open-dataset) (1.15.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.27.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow>=1.14.0->waymo-open-dataset) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.18.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (3.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.12.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14.0->waymo-open-dataset) (3.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14.0->waymo-open-dataset) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14.0->waymo-open-dataset) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.14.0->waymo-open-dataset) (46.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.14.0->waymo-open-dataset) (2.10.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=52067d5c0c45b7802eb8605a574a859d07fbd9c9221bb4e9f25d892980f85d8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: waymo-open-dataset, gast\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 waymo-open-dataset-1.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLbltjKuFWsM",
        "colab_type": "code",
        "outputId": "1998aeeb-bb80-4b75-84ae-123998d169c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0oM54zvjbUy",
        "colab_type": "code",
        "outputId": "2ba7fe84-1363-45c8-9ce6-0e421e08bbb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# required libs\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import inspect\n",
        "import tarfile\n",
        "import tqdm\n",
        "\n",
        "import random\n",
        "import math\n",
        "import itertools\n",
        "import functools\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.utils.data as tdata\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, datasets \n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "from waymo_open_dataset.utils import range_image_utils\n",
        "from waymo_open_dataset.utils import transform_utils\n",
        "from waymo_open_dataset.utils import  frame_utils\n",
        "from waymo_open_dataset.utils import  box_utils\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/utils/range_image_utils.py:59: The name tf.unsorted_segment_max is deprecated. Please use tf.math.unsorted_segment_max instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/waymo_open_dataset/utils/range_image_utils.py:226: The name tf.unsorted_segment_min is deprecated. Please use tf.math.unsorted_segment_min instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl5NK87yhyaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment to download train segment from Waymo google cloud into google drive\n",
        "# Training sets: training_0000.tar - training_0031.tar\n",
        "# Validation sets: validation_0000.tar - validation_0007.tar\n",
        "\n",
        "#!gsutil cp gs://waymo_open_dataset_v_1_0_0/training/training_0001.tar /content/drive/'My Drive'/Waymo_OD/\n",
        "\n",
        "def extract_from_tar(extract_from, extract_to):\n",
        "  file_tar = tarfile.open(name=extract_from, mode='r', fileobj=None, bufsize=10240)\"\n",
        "  file_tar.extractall(path=extract_to)\n",
        "\n",
        "\n",
        "#os.remove(r\"/content/drive/My Drive/Waymo_OD/training_0001.tar\")\n",
        "#os.remove(r\"/content/drive/My Drive/Waymo_OD/LICENSE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nu69FbQhv1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PointCloudParser(object):\n",
        "  \"\"\"Performs point cloud 3D-box filtering for a given set of classes.\"\"\"\n",
        "\n",
        "  def __init__(self, data_path, max_segments=None, cls_to_filter=(\"TYPE_PEDESTRIAN\", \"TYPE_VEHICLE\")):\n",
        "    \"\"\"\n",
        "    data_path: Data path to folder that containts Waymo TFRecords\n",
        "\n",
        "    max_segments: number of segments to process (None eq. all segments)\n",
        "\n",
        "    cls_to_filter: point clouds for these classes will be extracted\n",
        "    \"\"\"\n",
        "\n",
        "    assert os.path.exists(data_path), \"Existing path is required\"\n",
        "    assert np.all([os.path.splitext(file_name)[-1] == \".tfrecord\" \n",
        "                   for file_name in os.listdir(data_path)]), \"All files in the folder should be .tfrecord\"\n",
        "\n",
        "    self.data_path = data_path\n",
        "\n",
        "    self.cls_to_filter = cls_to_filter\n",
        "\n",
        "    self.segments_to_proc = os.listdir(self.data_path)[:max_segments]\n",
        "\n",
        "    self.type_enum = {\n",
        "                        \"TYPE_UNKNOWN\" : 0,\n",
        "                        \"TYPE_VEHICLE\" : 1,\n",
        "                        \"TYPE_PEDESTRIAN\" : 2,\n",
        "                        \"TYPE_SIGN\" : 3,\n",
        "                        \"TYPE_CYCLIST\" : 4\n",
        "                     }\n",
        "\n",
        "    self.code_2_type = {v:k for k,v in self.type_enum.items()}\n",
        "\n",
        "    self.obj_codes = tuple(self.type_enum[ctf] for ctf in self.cls_to_filter)\n",
        "  \n",
        "    self.bbox_count_dict = collections.defaultdict(int)\n",
        "\n",
        "  def start_processing(self, dest_path):\n",
        "    \"\"\"\n",
        "    Main routine method that starts points filtering. \n",
        "    Creates len(self.obj_codes) number folders in dest_path \n",
        "    for each class and saves point clouds for each box in .npy binary file. \n",
        "    \"\"\"\n",
        "\n",
        "    self.check_dir(dest_path)\n",
        "    self.dest_path = dest_path\n",
        "\n",
        "    for segment_name in self.segments_to_proc:\n",
        "      dataset = tf.data.TFRecordDataset(\n",
        "          os.path.join(self.data_path, segment_name), compression_type=\"\")\n",
        "      self.filter_frame_points(dataset)\n",
        "      \n",
        "\n",
        "  def filter_frame_points(self, dataset, min_pts_threshold=75):\n",
        "    \"\"\"\n",
        "    Return dict that maps from object type ids to point clouds. \n",
        "    Value for each key is a list of numpy arrays, \n",
        "    where each numpy array containts points for a single 3D bbox.\n",
        "    \"\"\"\n",
        "\n",
        "    bbox_dict = {obj:[] for obj in self.obj_codes}\n",
        "\n",
        "    for data in tqdm.tqdm_notebook(dataset):\n",
        "      frame = open_dataset.Frame()\n",
        "      \n",
        "      frame.ParseFromString(bytearray(data.numpy()))\n",
        "\n",
        "      frame_pts = self.frame_points(frame)\n",
        "\n",
        "      for obj_code in self.obj_codes:\n",
        "        boxes = self.frame_boxes_3D(frame, obj_code)\n",
        "\n",
        "        if len(boxes):\n",
        "          bbox_dict[obj_code].extend(\n",
        "              \n",
        "               [\n",
        "                filt_pts for filt_pts in (tf.boolean_mask(frame_pts, mask).numpy() \n",
        "                                          for mask in tf.transpose(\n",
        "                                              box_utils.is_within_box_3d(frame_pts, boxes)\n",
        "                                          )) \n",
        "                if len(filt_pts) >= min_pts_threshold\n",
        "               ]\n",
        "          )\n",
        "    \n",
        "    self.save_point_cloud(bbox_dict)\n",
        "    \n",
        "\n",
        "  def frame_points(self, frame, projection=False):\n",
        "    \"\"\"\n",
        "    Returns Tensor of points in a vehicle coord. system for a given frame. \n",
        "    If projection is True => returns points projection on frame\n",
        "    \"\"\"\n",
        "    (range_images, camera_projections, range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(frame)\n",
        "\n",
        "    points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
        "        frame,\n",
        "        range_images,\n",
        "        camera_projections,\n",
        "        range_image_top_pose,\n",
        "        ri_index=0)\n",
        "\n",
        "    if projection:\n",
        "      return tf.constant(np.concatenate(cp_points))\n",
        "    else:\n",
        "      return tf.constant(np.concatenate(points, axis=0))\n",
        "\n",
        "\n",
        "  def frame_boxes_3D(self, frame, obj_code):\n",
        "    \"\"\"\n",
        "    Returns np.array of box descriptors for a given frame\n",
        "    \"\"\"\n",
        "\n",
        "    box_stats = np.array([tf.constant([lable.box.center_x, lable.box.center_y, lable.box.center_z, \n",
        "                  lable.box.width, lable.box.length, lable.box.height, lable.box.heading])\n",
        "                  for lable in frame.laser_labels if lable.type == obj_code])\n",
        "\n",
        "    return box_stats\n",
        "\n",
        "\n",
        "  def check_dir(self, dest_path):\n",
        "    \"\"\"Creates required dir. for files\"\"\"\n",
        "    if not os.path.exists(dest_path):\n",
        "      os.mkdir(dest_path)\n",
        "    for obj_code in self.obj_codes:\n",
        "      obj_folder = os.path.join(dest_path, self.code_2_type[obj_code])\n",
        "      if not os.path.exists(obj_folder):\n",
        "        os.mkdir(obj_folder)\n",
        "\n",
        "\n",
        "  def save_point_cloud(self, bbox_dict):\n",
        "    \"\"\"Saves filtered point cloud (numpy array of size [N, 3]) into .npy binary file.\"\"\"\n",
        "\n",
        "    print(\"Saving.\")\n",
        "    \n",
        "    for obj_code in bbox_dict.keys():\n",
        "      for bbox in bbox_dict[obj_code]:\n",
        "        np.save(\n",
        "                  os.path.join(self.dest_path, self.code_2_type[obj_code], \n",
        "                              \"{0}.npy\".format(self.bbox_count_dict[obj_code]).zfill(12)), \n",
        "                  bbox\n",
        "               )\n",
        "        self.bbox_count_dict[obj_code] += 1\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Y584c9Nhxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Point cloud processing functions\n",
        "# segments_path = \"/content/drive/My Drive/Waymo_OD/\"\n",
        "# testing_pcp = PointCloudParser(segments_path)\n",
        "# testing_pcp.start_processing(\"/content/drive/My Drive/PointNet/Data_folder_train_0001\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yra0BHHZcYnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shutil.rmtree(\"/content/drive/My Drive/PointNet/Data_folder_train_0001\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWlmN3ejq3N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def box_metadata_parser(dataset, dataset_id, obj_type=\"TYPE_PEDESTRIAN\", save=False):\n",
        "  \"\"\"\n",
        "  Compute box metadata from a given segment.\n",
        "  Box metadata includes box track_id and L2 distance to box in vehicle coord. system\n",
        "  \"\"\"\n",
        "  df_dict_template = {\"track_id\" : [], \"box_dist\" : []}\n",
        "\n",
        "  type_enum = {\n",
        "      \"TYPE_UNKNOWN\" : 0,\n",
        "      \"TYPE_VEHICLE\" : 1,\n",
        "      \"TYPE_PEDESTRIAN\" : 2,\n",
        "      \"TYPE_SIGN\" : 3,\n",
        "      \"TYPE_CYCLIST\" : 4,\n",
        "    }\n",
        "\n",
        "  obj_code = type_enum[obj_type]\n",
        "\n",
        "  for data in dataset:\n",
        "    frame = open_dataset.Frame()\n",
        "    frame.ParseFromString(bytearray(data.numpy()))\n",
        "    for lable in frame.laser_labels:\n",
        "      if lable.type == obj_code:\n",
        "        df_dict_template[\"track_id\"].append(lable.id)\n",
        "        df_dict_template[\"box_dist\"].append(np.linalg.norm([lable.box.center_x, lable.box.center_y, lable.box.center_z]))\n",
        "\n",
        "  metadata_df = pd.DataFrame.from_dict(df_dict_template)\n",
        "\n",
        "  if save:\n",
        "    if not os.path.exists(\"./box_metadata\"):\n",
        "      os.mkdir(\"./box_metadata\")\n",
        "    \n",
        "    metadata_df.to_csv(\".//box_metadata//metadata_{0}_{1}.csv\".format(obj_type, dataset_id), index = False, header=True)\n",
        "\n",
        "  return metadata_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KtOYl7ytcZt",
        "colab_type": "text"
      },
      "source": [
        "### Create data loaders for train and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mkPc734kjAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MinPointSampler(object):\n",
        "  \"\"\"Transfromation that samples pts_num number of points from the input point cloud\"\"\"\n",
        "  def __init__(self, pts_num):\n",
        "    self.pts_num = pts_num\n",
        "\n",
        "  def __call__(self, point_cloud):\n",
        "    return point_cloud[np.random.choice(point_cloud.shape[0], size=self.pts_num, replace=False), :]\n",
        "\n",
        "\n",
        "class PointScaler(object):\n",
        "  \"\"\"Scales point cloud to a new one with mean = 0 and maximum vector length of 1\"\"\"\n",
        "  def __call__(self, point_cloud): \n",
        "    return (point_cloud - np.mean(point_cloud, axis=0)) / np.linalg.norm(point_cloud, axis=1).max()\n",
        "\n",
        "\n",
        "class RndPointsAugmentations(object):\n",
        "  \"\"\"\n",
        "  Performs 2 types of point data augmentation: \n",
        "  Random jittering via uniformly distributed noise and random rotation along z-axis\n",
        "  \"\"\"\n",
        "  def __init__(self, jitter_a=0, jitter_b=0.2):\n",
        "    self.jitter_a = jitter_a\n",
        "    self.jitter_b = jitter_b\n",
        "    \n",
        "  def __call__(self, point_cloud):\n",
        "    theta = np.random.uniform(0, np.pi*2)\n",
        "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n",
        "    point_cloud[:, :2] = point_cloud[:, :2].dot(rotation_matrix)\n",
        "    return point_cloud + np.random.normal(self.jitter_a, self.jitter_b, size=point_cloud.shape)\n",
        "\n",
        "\n",
        "def create_train_val_data_loaders(data_dir, *, min_pts=75, batch_size=32, validation_frac = 0.2, num_of_workers=0):\n",
        "    \"\"\"\n",
        "    Return pair of pytorch dataloaders for train and validation sets.\n",
        "    \"\"\"\n",
        "    # sample => scale => (if train) random jitter and random rotation along z axis => transform to Pytorch tensor \n",
        "\n",
        "    mps_transform = MinPointSampler(min_pts)\n",
        "    pt_scaler = PointScaler()\n",
        "    points_aug = RndPointsAugmentations()\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "                                           mps_transform,\n",
        "                                           points_aug,\n",
        "                                           pt_scaler,\n",
        "                                           transforms.ToTensor()\n",
        "                                          ])\n",
        "    \n",
        "    val_transforms = transforms.Compose([\n",
        "                                        mps_transform,\n",
        "                                        pt_scaler,\n",
        "                                        transforms.ToTensor()\n",
        "                                       ])\n",
        "    \n",
        "    train_data = datasets.DatasetFolder(data_dir, loader=np.load, extensions=(\"npy\"), \n",
        "                                        transform=train_transforms)\n",
        "\n",
        "    val_data = datasets.DatasetFolder(data_dir, loader=np.load, extensions=(\"npy\"),\n",
        "                                      transform=val_transforms)\n",
        "        \n",
        "    dataset_len = len(train_data)\n",
        "\n",
        "    indices = np.arange(dataset_len)\n",
        "    \n",
        "    val_abs_size = np.int(np.floor(validation_frac * dataset_len))\n",
        "\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    train_id, val_id = indices[val_abs_size:], indices[:val_abs_size]\n",
        "    \n",
        "    train_sampler = tdata.SubsetRandomSampler(train_id)\n",
        "    \n",
        "    val_sampler = tdata.SubsetRandomSampler(val_id)\n",
        "    \n",
        "    train_loader = tdata.DataLoader(train_data,\n",
        "                   sampler=train_sampler, batch_size=batch_size, num_workers=num_of_workers)\n",
        "    \n",
        "    val_loader = tdata.DataLoader(val_data,\n",
        "                   sampler=val_sampler, batch_size=batch_size, num_workers=num_of_workers)\n",
        "    \n",
        "    return train_loader, val_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp5dG6VCHiA6",
        "colab_type": "text"
      },
      "source": [
        "### PointNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FBqxmCKHpxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class STN3d(nn.Module):\n",
        "  \"\"\"TNet for 3D point transformation\"\"\"\n",
        "  def __init__(self):\n",
        "      super(STN3d, self).__init__()\n",
        "      self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "      self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "      self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "      self.fc1 = nn.Linear(1024, 512)\n",
        "      self.fc2 = nn.Linear(512, 256)\n",
        "      self.fc3 = nn.Linear(256, 9)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      batchsize = x.size()[0]\n",
        "      x = F.relu(self.bn1(self.conv1(x)))\n",
        "      x = F.relu(self.bn2(self.conv2(x)))\n",
        "      x = F.relu(self.bn3(self.conv3(x)))\n",
        "      x = torch.max(x, 2, keepdim=True)[0]\n",
        "      x = x.view(-1, 1024)\n",
        "\n",
        "      x = F.relu(self.bn4(self.fc1(x)))\n",
        "      x = F.relu(self.bn5(self.fc2(x)))\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n",
        "      if x.is_cuda:\n",
        "          iden = iden.cuda()\n",
        "      x = x + iden\n",
        "      x = x.view(-1, 3, 3)\n",
        "      return x\n",
        "\n",
        "\n",
        "class STNkd(nn.Module):\n",
        "  \"\"\"Tnet for k-D point transformation\"\"\"\n",
        "  def __init__(self, k=64):\n",
        "      super(STNkd, self).__init__()\n",
        "      self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
        "      self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "      self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "      self.fc1 = nn.Linear(1024, 512)\n",
        "      self.fc2 = nn.Linear(512, 256)\n",
        "      self.fc3 = nn.Linear(256, k*k)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.bn4 = nn.BatchNorm1d(512)\n",
        "      self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "      self.k = k\n",
        "\n",
        "  def forward(self, x):\n",
        "      batchsize = x.size()[0]\n",
        "      x = F.relu(self.bn1(self.conv1(x)))\n",
        "      x = F.relu(self.bn2(self.conv2(x)))\n",
        "      x = F.relu(self.bn3(self.conv3(x)))\n",
        "      x = torch.max(x, 2, keepdim=True)[0]\n",
        "      x = x.view(-1, 1024)\n",
        "\n",
        "      x = F.relu(self.bn4(self.fc1(x)))\n",
        "      x = F.relu(self.bn5(self.fc2(x)))\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
        "      if x.is_cuda:\n",
        "          iden = iden.cuda()\n",
        "      x = x + iden\n",
        "      x = x.view(-1, self.k, self.k)\n",
        "      return x\n",
        "\n",
        "class PointNetfeat(nn.Module):\n",
        "  \"\"\"PointNet features part\"\"\"\n",
        "  def __init__(self, global_feat = True, feature_transform = False):\n",
        "      super(PointNetfeat, self).__init__()\n",
        "      self.stn = STN3d()\n",
        "      self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "      self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "      self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "      self.bn1 = nn.BatchNorm1d(64)\n",
        "      self.bn2 = nn.BatchNorm1d(128)\n",
        "      self.bn3 = nn.BatchNorm1d(1024)\n",
        "      self.global_feat = global_feat\n",
        "      self.feature_transform = feature_transform\n",
        "      if self.feature_transform:\n",
        "          self.fstn = STNkd(k=64)\n",
        "\n",
        "  def forward(self, x):\n",
        "      n_pts = x.size()[2]\n",
        "      trans = self.stn(x)\n",
        "      x = x.transpose(2, 1)\n",
        "      x = torch.bmm(x, trans)\n",
        "      x = x.transpose(2, 1)\n",
        "      x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "      if self.feature_transform:\n",
        "          trans_feat = self.fstn(x)\n",
        "          x = x.transpose(2,1)\n",
        "          x = torch.bmm(x, trans_feat)\n",
        "          x = x.transpose(2,1)\n",
        "      else:\n",
        "          trans_feat = None\n",
        "\n",
        "      pointfeat = x\n",
        "      x = F.relu(self.bn2(self.conv2(x)))\n",
        "      x = self.bn3(self.conv3(x))\n",
        "      x = torch.max(x, 2, keepdim=True)[0]\n",
        "      x = x.view(-1, 1024)\n",
        "      if self.global_feat:\n",
        "          return x, trans, trans_feat\n",
        "      else:\n",
        "          x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
        "          return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
        "\n",
        "class DeepCosineMetric(nn.Module):\n",
        "  \"\"\"Deep cosine metric net-end\"\"\"\n",
        "  def __init__(self, k=2, scale_param=1):\n",
        "    super(DeepCosineMetric, self).__init__()\n",
        "    self.fc_feature_last = nn.Linear(256, 128, bias=False)\n",
        "    self.fc_last = nn.Linear(128, k, bias=False)\n",
        "    self.scale_param = scale_param\n",
        "  \n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.fc_feature_last(x)\n",
        "\n",
        "    x = F.normalize(x, p=2, dim=1, eps=1e-12)\n",
        "\n",
        "    descriptor = x.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      self.fc_last.weight.div_(torch.norm(self.fc_last.weight, dim=1, keepdim=True))\n",
        "\n",
        "    x = self.scale_param*self.fc_last(x)\n",
        "\n",
        "    return F.log_softmax(x, dim=1), descriptor\n",
        "\n",
        "\n",
        "\n",
        "class PointNetCls(nn.Module):\n",
        "  \"\"\"PointNet for classification\"\"\"\n",
        "  def __init__(self, k=2, feature_transform=False):\n",
        "      super(PointNetCls, self).__init__()\n",
        "      self.feature_transform = feature_transform\n",
        "      self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform)\n",
        "      self.fc1 = nn.Linear(1024, 512)\n",
        "      self.fc2 = nn.Linear(512, 256)\n",
        "      self.deep_cosine_metric = DeepCosineMetric(k=k)\n",
        "\n",
        "      self.dropout = nn.Dropout(p=0.3)\n",
        "      self.bn1 = nn.BatchNorm1d(512)\n",
        "      self.bn2 = nn.BatchNorm1d(256)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      x, trans, trans_feat = self.feat(x)\n",
        "      x = F.relu(self.bn1(self.fc1(x)))\n",
        "      x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
        "      x, descriptor = self.deep_cosine_metric(x)\n",
        "      return x, trans, trans_feat, descriptor\n",
        "\n",
        "\n",
        "\n",
        "def feature_transform_regularizer(trans):\n",
        "  \"\"\"Regularization for TNet (TNet transformation matrix should be close to orthogonal)\"\"\"\n",
        "  d = trans.size()[1]\n",
        "  batchsize = trans.size()[0]\n",
        "  I = torch.eye(d)[None, :, :]\n",
        "  if trans.is_cuda:\n",
        "    I = I.to(\"cuda\")\n",
        "  loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHWQPLaEHqVh",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su7CUmUuappM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparams\n",
        "\n",
        "manual_seed = 424242                        # seed for rng\n",
        "cls_num = 2                                 # number of classes, should match that number in dataloader \n",
        "pointnet_feature_transform = True           # Use T-Net block for inner features transformation\n",
        "existing_model_path = \"\"                    # download model state from path (\"\" if no pretrained model to use)\n",
        "epoch_number = 1                            # epoch to train  \n",
        "batch_size = 32                             # should batch match batch size in dataloader\n",
        "val_step = 10                               # evaluate network on validation set example if cur_step % val_step == 0\n",
        "reg_lambda = 0.001                          # coefficient for T-Net orthogonal regularization\n",
        "save_model_to = \"/content/drive/My Drive/PointNet/Model/\"    # path to folder where model will be saved"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTSD1TLAf18I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "adam_parameters = {\"lr\": 0.001, \"betas\": (0.9, 0.999)}\n",
        "shelduler_parameters = {\"step_size\": 20, \"gamma\": 0.5}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQrWn1B0INlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "\n",
        "point_net_cls = PointNetCls(k=cls_num, feature_transform=pointnet_feature_transform)\n",
        "\n",
        "train_loader, val_loader = create_train_val_data_loaders(\"/content/drive/My Drive/PointNet/Data_folder_train_0001/\", \n",
        "                                                         num_of_workers=2)\n",
        "\n",
        "if existing_model_path != \"\":\n",
        "    point_net_cls.load_state_dict(torch.load(existing_model_path))\n",
        "\n",
        "optimizer = optim.Adam(point_net_cls.parameters(), **adam_parameters)\n",
        "\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, **shelduler_parameters)\n",
        "\n",
        "point_net_cls.cuda()\n",
        "\n",
        "num_batches = len(train_loader)\n",
        "\n",
        "for epoch in range(epoch_number):\n",
        "    scheduler.step()\n",
        "\n",
        "    for i, [points, target] in enumerate(train_loader):\n",
        "\n",
        "        points = points.squeeze().transpose(2, 1)\n",
        "\n",
        "        points, target = points.to(device, dtype=torch.float), target.to(device) # tensors to GPU\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        point_net_cls = point_net_cls.train()\n",
        "\n",
        "        pred, trans, trans_feat, _ = point_net_cls(points)\n",
        "\n",
        "        loss = F.nll_loss(pred, target)\n",
        "        \n",
        "        if pointnet_feature_transform:\n",
        "            loss += feature_transform_regularizer(trans_feat) * reg_lambda\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        pred_choice = pred.data.max(1)[1]\n",
        "\n",
        "        correct = pred_choice.eq(target.data).cpu().sum()\n",
        "\n",
        "        print('[epoch #{0}: {1}/{2}] train loss: {3} batch accuracy: {4}'.format(epoch, i, num_batches, loss.item(), correct.item() / float(batch_size)))\n",
        "\n",
        "        if i % val_step == 0:\n",
        "            j, [points, target] = next(enumerate(val_loader))\n",
        "\n",
        "            points = points.squeeze().transpose(2, 1)\n",
        "\n",
        "            points, target = points.cuda(), target.cuda()\n",
        "\n",
        "            point_net_cls = point_net_cls.eval()\n",
        "\n",
        "            pred, _, _, _ = point_net_cls(points)\n",
        "\n",
        "            loss = F.nll_loss(pred, target)\n",
        "\n",
        "            pred_choice = pred.data.max(1)[1]\n",
        "\n",
        "            correct = pred_choice.eq(target.data).cpu().sum()\n",
        "\n",
        "            print('[epoch #{0}: {1}/{2}] validation loss: {3} batch accuracy: {4}'.format(epoch, i, num_batches, loss.item(), correct.item()/float(batch_size)))\n",
        "\n",
        "        torch.save(point_net_cls.state_dict(), os.path.join(save_model_to, \"cls_model_test_{0}.pth\".format(epoch)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGGmey6iHoN9",
        "colab_type": "code",
        "outputId": "acd37fb3-31a4-489e-f213-4ab2e291d175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing cosine metric on the validation set\n",
        "\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "\n",
        "point_net_cls = PointNetCls(k=cls_num, feature_transform=pointnet_feature_transform)\n",
        "\n",
        "point_net_cls.load_state_dict(torch.load(\"/content/drive/My Drive/PointNet/Model/cls_model_test_0.pth\"))\n",
        "\n",
        "train_loader, val_loader = create_train_val_data_loaders(\"/content/drive/My Drive/PointNet/Data_folder_train_0001/\", \n",
        "                                                         num_of_workers=2)\n",
        "\n",
        "val_loader.dataset.class_to_idx['TYPE_PEDESTRIAN']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsdlW2E4Ij7M",
        "colab_type": "code",
        "outputId": "03620099-0d2b-4455-f2b3-c1a7a7fcc67e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "62a1bf53601a46c8b4a817d6b0aeeb32",
            "59c645f882ab4490974496f6324cdc90",
            "1c914c1ae270410aaae8521f6c3dc4a8",
            "b0b34bfe985a4efe8cba415d0b04054a",
            "670d688c6ba94279acc92d4a64384998",
            "bc93734c5b4f4b809b3d2ea4c306157a",
            "38174a495cd44616b02d76f0073e2935",
            "78820c6067714191b2678d4f0995ba98"
          ]
        }
      },
      "source": [
        "point_net_cls.to(device)\n",
        "\n",
        "deep_cosine_descriptor = []\n",
        "predictions = []\n",
        "targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for [points, target]  in tqdm.tqdm_notebook(val_loader):\n",
        "        points = points.squeeze().transpose(2, 1)\n",
        "        points, target = points.cuda(), target.cuda()\n",
        "        pred, _, _, descriptor = point_net_cls(points)\n",
        "        predictions.append(pred.data.max(1)[1])\n",
        "        deep_cosine_descriptor.append(descriptor)\n",
        "        targets.append(target)\n",
        "\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62a1bf53601a46c8b4a817d6b0aeeb32",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=126), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "452HPDRFVCzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "concat_descriptors = np.vstack(list(map(lambda x: x.cpu().numpy(), deep_cosine_descriptor)))\n",
        "concat_targets = np.concatenate(list(map(lambda x: x.cpu().numpy(), targets)))\n",
        "concat_predictions = np.concatenate(list(map(lambda x: x.cpu().numpy(), predictions)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyp82b8iBkAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(os.path.join(\"/content/drive/My Drive/PointNet/Deep_cosine_metric_tests\", \"pred.npy\"), np.array(concat_predictions))    \n",
        "np.save(os.path.join(\"/content/drive/My Drive/PointNet/Deep_cosine_metric_tests\", \"targets.npy\"), np.array(concat_targets))\n",
        "np.save(os.path.join(\"/content/drive/My Drive/PointNet/Deep_cosine_metric_tests\", \"deep_cosine_descriptor.npy\"), np.array(concat_descriptors))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqBcJS8QUaP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.load(os.path.join(\"/content/drive/My Drive/PointNet/Deep_cosine_metric_tests\", \"pred.npy\"), allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki-B97f0IaFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_pedestrian_descriptors = concat_descriptors[np.logical_and(concat_targets == concat_predictions, concat_targets == 0)]\n",
        "\n",
        "true_pedestrian_cosine_distance_matrix = 1 - cosine_similarity(true_pedestrian_descriptors)\n",
        "\n",
        "true_pedestrian_mean, true_pedestrian_std, true_pedestrian_median = np.mean(true_pedestrian_cosine_distance_matrix), np.std(true_pedestrian_cosine_distance_matrix), np.median(true_pedestrian_cosine_distance_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAmXa6mShPH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "false_pedestrian_descriptors = concat_descriptors[np.logical_and(concat_targets != concat_predictions, concat_targets == 1)]\n",
        "\n",
        "false_pedestrian_cosine_distance_matrix = 1 - cosine_similarity(false_pedestrian_descriptors)\n",
        "\n",
        "false_pedestrian_mean, false_pedestrian_std, false_pedestrian_median = np.mean(false_pedestrian_cosine_distance_matrix), np.std(false_pedestrian_cosine_distance_matrix), np.median(false_pedestrian_cosine_distance_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHwMhksPItaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "bc6ae8a3-04ea-443a-fdcd-af46139ae208"
      },
      "source": [
        "# Cosine distance stats\n",
        "\n",
        "pd.DataFrame({\"Mean\" : [true_pedestrian_mean, false_pedestrian_mean], \"Std\" : true_pedestrian_std, \"Median\" : true_pedestrian_median}, \n",
        "             index=[\"True pedestrian detections cosine distance\"])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean</th>\n",
              "      <th>Std</th>\n",
              "      <th>Median</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TP pedestrian detections</th>\n",
              "      <td>0.059364</td>\n",
              "      <td>0.085245</td>\n",
              "      <td>0.038902</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Mean       Std    Median\n",
              "TP pedestrian detections  0.059364  0.085245  0.038902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}